import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load model (open source, adjust model name if needed)
model_name = "TheBloke/Mistral-7B-Instruct-GGML"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

# Load system prompt
with open("system_prompt.txt", "r", encoding="utf-8") as f:
    system_prompt = f.read()

def ask_ai(question):
    prompt = f"{system_prompt}\nSoalan: {question}\nJawapan:"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=200)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Buang prompt asal dari jawapan
    answer = answer.replace(prompt, "").strip()
    return answer

# Gradio interface
iface = gr.Interface(
    fn=ask_ai,
    inputs=gr.Textbox(label="Tanya AI"),
    outputs=gr.Textbox(label="Jawapan AI"),
    title="AI Jawapan TEGAS & MANUSIAWI",
    description="AI khas jawab ringkas, padat, manusiawi, moral"
)

iface.launch(server_name="0.0.0.0", server_port=7860)
